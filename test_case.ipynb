{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_HOME=/data/cuda/11.8\n",
      "export PATH=$CUDA_HOME/bin:${PATH}\n",
      "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:${LD_LIBRARY_PATH}\n",
      "\n",
      "export TENSORRT_HOME=/data/cuda/TensorRT-8.6.1.6\n",
      "export PATH=${TENSORRT_HOME}/bin:${PATH}\n",
      "export LD_LIBRARY_PATH=${TENSORRT_HOME}/lib:${LD_LIBRARY_PATH}\n"
     ]
    }
   ],
   "source": [
    "# ENV\n",
    "# Driver Version: 510.47.03 \n",
    "# CUDA Version: 11.8 \n",
    "# TensorRT Version: 8.6.1\n",
    "!cat path.sh\n",
    "!source path.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define the bad case model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CaseTest(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "  \n",
    "  def forward(self, x: torch.Tensor):\n",
    "    max_length = x.max()\n",
    "    y = torch.arange(max_length)\n",
    "    return y\n",
    "  \n",
    "model = CaseTest()\n",
    "model.eval()\n",
    "input_names = [ \"x\" ]\n",
    "    \n",
    "input_x = torch.Tensor([[x for x in range(10)]]).to(torch.int64)  # B x T\n",
    "print(input_x, input_x.shape)\n",
    "torch.onnx.export(model, \n",
    "                  (input_x, ), \n",
    "                  \"onnx_export_badcase.onnx\",\n",
    "                  export_params=True,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names=input_names,\n",
    "                  output_names=['y'],\n",
    "                  dynamic_axes={\n",
    "                    'x':{0:'B', 1:'T_0'}\n",
    "                  },\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[I] RUNNING | Command: /data/k2/miniconda3/envs/tts_env/bin/polygraphy surgeon sanitize onnx_export_badcase.onnx --fold-constants -o onnx_export_badcase.folded.onnx\n",
      "[I] Loading model: /data/k2/tts-latest/haoyue/onnx_export_badcase.onnx\n",
      "[I] Original Model:\n",
      "    Name: torch_jit | ONNX Opset: 14\n",
      "    \n",
      "    ---- 1 Graph Input(s) ----\n",
      "    {x [dtype=int64, shape=('B', 'T_0')]}\n",
      "    \n",
      "    ---- 1 Graph Output(s) ----\n",
      "    {y [dtype=int64, shape=('Range_4_o0__d0',)]}\n",
      "    \n",
      "    ---- 0 Initializer(s) ----\n",
      "    \n",
      "    ---- 5 Node(s) ----\n",
      "    \n",
      "[I] Folding Constants | Pass 1\n",
      "[I]     Total Nodes | Original:     5, After Folding:     3 |     2 Nodes Folded\n",
      "[I] Folding Constants | Pass 2\n",
      "[I]     Total Nodes | Original:     3, After Folding:     3 |     0 Nodes Folded\n",
      "[I] Saving ONNX model to: onnx_export_badcase.folded.onnx\n",
      "[I] New Model:\n",
      "    Name: torch_jit | ONNX Opset: 14\n",
      "    \n",
      "    ---- 1 Graph Input(s) ----\n",
      "    {x [dtype=int64, shape=('B', 'T_0')]}\n",
      "    \n",
      "    ---- 1 Graph Output(s) ----\n",
      "    {y [dtype=int64, shape=('Range_2_o0__d0',)]}\n",
      "    \n",
      "    ---- 2 Initializer(s) ----\n",
      "    \n",
      "    ---- 3 Node(s) ----\n",
      "    \n",
      "[I] PASSED | Runtime: 7.028s | Command: /data/k2/miniconda3/envs/tts_env/bin/polygraphy surgeon sanitize onnx_export_badcase.onnx --fold-constants -o onnx_export_badcase.folded.onnx\n",
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[I] Loading model: /data/k2/tts-latest/haoyue/onnx_export_badcase.folded.onnx\n",
      "[I] ==== ONNX Model ====\n",
      "    Name: torch_jit | ONNX Opset: 14\n",
      "    \n",
      "    ---- 1 Graph Input(s) ----\n",
      "    {x [dtype=int64, shape=('B', 'T_0')]}\n",
      "    \n",
      "    ---- 1 Graph Output(s) ----\n",
      "    {y [dtype=int64, shape=('Range_2_o0__d0',)]}\n",
      "    \n",
      "    ---- 2 Initializer(s) ----\n",
      "    {/Constant_output_0 [dtype=int64, shape=()],\n",
      "     /Constant_1_output_0 [dtype=int64, shape=()]}\n",
      "    \n",
      "    ---- 3 Node(s) ----\n",
      "    Node 0    | /ReduceMax [Op: ReduceMax]\n",
      "        {x [dtype=int64, shape=('B', 'T_0')]}\n",
      "         -> {/ReduceMax_output_0 [dtype=int64, shape=()]}\n",
      "    \n",
      "    Node 1    | /Cast [Op: Cast]\n",
      "        {/ReduceMax_output_0 [dtype=int64, shape=()]}\n",
      "         -> {/Cast_output_0 [dtype=int64, shape=()]}\n",
      "    \n",
      "    Node 2    | /Range [Op: Range]\n",
      "        {Initializer | /Constant_output_0 [dtype=int64, shape=()],\n",
      "         /Cast_output_0 [dtype=int64, shape=()],\n",
      "         Initializer | /Constant_1_output_0 [dtype=int64, shape=()]}\n",
      "         -> {y [dtype=int64, shape=('Range_2_o0__d0',)]}\n"
     ]
    }
   ],
   "source": [
    "# Folded the model and print out model topo in onnx format\n",
    "!polygraphy surgeon sanitize onnx_export_badcase.onnx --fold-constants -o onnx_export_badcase.folded.onnx\n",
    "!polygraphy inspect model onnx_export_badcase.folded.onnx --show layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])}\n",
      "['y']\n",
      "[array([0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# Run this model in onnxruntime with cuda provider\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "cuda_providers = [\n",
    "      ('CUDAExecutionProvider', {\n",
    "      }),\n",
    "      ('CPUExecutionProvider', {\n",
    "        'intra_op_num_threads': 1,\n",
    "        'inter_op_num_threads': 32,\n",
    "      })\n",
    "]\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "onnx_model = ort.InferenceSession(\"onnx_export_badcase.folded.onnx\", sess_options, providers = cuda_providers)\n",
    "\n",
    "model_inputs = {\"x\": input_x.numpy()}\n",
    "print(model_inputs)\n",
    "output_names = [n.name for n in onnx_model.get_outputs()]\n",
    "print(output_names)\n",
    "model_output = onnx_model.run(output_names, model_inputs)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])}\n",
      "['y']\n",
      "[array([], dtype=int64)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 11:39:18.179663341 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2023-09-22 03:39:18 WARNING] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "2023-09-22 11:39:18.179831832 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2023-09-22 03:39:18 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2023-09-22 11:39:18.184192065 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2023-09-22 03:39:18 WARNING] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "2023-09-22 11:39:18.190826745 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2023-09-22 03:39:18 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2023-09-22 11:39:18.396060439 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2023-09-22 03:39:18 WARNING] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    }
   ],
   "source": [
    "# Run this model in onnxruntime with TensorRT provider\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "cuda_providers = [\n",
    "      ('TensorrtExecutionProvider', {\n",
    "      }),\n",
    "      ('CUDAExecutionProvider', {\n",
    "      }),\n",
    "      ('CPUExecutionProvider', {\n",
    "        'intra_op_num_threads': 1,\n",
    "        'inter_op_num_threads': 32,\n",
    "      })\n",
    "]\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "onnx_model = ort.InferenceSession(\"onnx_export_badcase.folded.onnx\", sess_options, providers = cuda_providers)\n",
    "\n",
    "model_inputs = {\"x\": input_x.numpy()}\n",
    "print(model_inputs)\n",
    "output_names = [n.name for n in onnx_model.get_outputs()]\n",
    "print(output_names)\n",
    "model_output = onnx_model.run(output_names, model_inputs)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we see that the output of model with cuda provider nor tensorrt provider is different.\n",
    "- For CUDA provider, y = array([0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=int64)\n",
    "- For TensorRT provider, y = array([], dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems that there maybe some error in the TensorRT Graph optimization, so I run trtexec to get the TensorRT engine LayerInfo.\n",
    "- Run trtexec cmd outside notebook.\n",
    "```\n",
    "!trtexec \\\n",
    "  --onnx=onnx_export_badcase.folded.onnx \\\n",
    "  --saveEngine=onnx_export_badcase.folded.trt \\\n",
    "  --nvtxMode=verbose \\\n",
    "  --exportLayerInfo=onnx_export_badcase.folded.json \\\n",
    "  --optShapes=x:1x77\n",
    "```\n",
    "- Get the graph topo of TensorRT engine: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Layers\": [{\n",
      "  \"Name\": \"[trainStation1]\",\n",
      "  \"LayerType\": \"TrainStation\",\n",
      "  \"Inputs\": [],\n",
      "  \"Outputs\": [],\n",
      "  \"TacticValue\": \"0x0000000000000000\",\n",
      "  \"StreamId\": 0,\n",
      "  \"Metadata\": \"\"\n",
      "},{\n",
      "  \"Name\": \"/ReduceMax\",\n",
      "  \"LayerType\": \"Reduce\",\n",
      "  \"Inputs\": [\n",
      "  {\n",
      "    \"Name\": \"x\",\n",
      "    \"Location\": \"Device\",\n",
      "    \"Dimensions\": [1,77],\n",
      "    \"Format/Datatype\": \"Row major linear Int32\"\n",
      "  }],\n",
      "  \"Outputs\": [\n",
      "  {\n",
      "    \"Name\": \"/ReduceMax_output_0\",\n",
      "    \"Location\": \"Device\",\n",
      "    \"Dimensions\": [],\n",
      "    \"Format/Datatype\": \"Row major linear Int32\"\n",
      "  }],\n",
      "  \"ParameterType\": \"Reduce\",\n",
      "  \"Operation\": \"MAX\",\n",
      "  \"ReduceAxes\": [1,1],\n",
      "  \"KeepDimensions\": 0,\n",
      "  \"TacticValue\": \"0x0000000000000001\",\n",
      "  \"StreamId\": 0,\n",
      "  \"Metadata\": \"[ONNX Layer: /ReduceMax]\"\n",
      "},{\n",
      "  \"Name\": \"/ReduceMax_output_0[DevicetoShapeHostCopy]\",\n",
      "  \"LayerType\": \"DeviceToShapeHost\",\n",
      "  \"Inputs\": [\n",
      "  {\n",
      "    \"Name\": \"/ReduceMax_output_0\",\n",
      "    \"Location\": \"Device\",\n",
      "    \"Dimensions\": [],\n",
      "    \"Format/Datatype\": \"Row major linear Int32\"\n",
      "  }],\n",
      "  \"Outputs\": [],\n",
      "  \"TacticValue\": \"0x0000000000000000\",\n",
      "  \"StreamId\": 0,\n",
      "  \"Metadata\": \"\"\n",
      "},{\n",
      "  \"Name\": \"[trainStation2]\",\n",
      "  \"LayerType\": \"TrainStation\",\n",
      "  \"Inputs\": [],\n",
      "  \"Outputs\": [],\n",
      "  \"TacticValue\": \"0x0000000000000000\",\n",
      "  \"StreamId\": 0,\n",
      "  \"Metadata\": \"\"\n",
      "},{\n",
      "  \"Name\": \"/Range\",\n",
      "  \"LayerType\": \"Fill\",\n",
      "  \"Inputs\": [],\n",
      "  \"Outputs\": [\n",
      "  {\n",
      "    \"Name\": \"y\",\n",
      "    \"Location\": \"Device\",\n",
      "    \"Dimensions\": [0],\n",
      "    \"Format/Datatype\": \"Row major linear Int32\"\n",
      "  }],\n",
      "  \"ParameterType\": \"Fill\",\n",
      "  \"Op\": \"LINSPACE\",\n",
      "  \"dimension\": [-2147483647],\n",
      "  \"alpha\": 0,\n",
      "  \"beta\": 1,\n",
      "  \"TacticValue\": \"0x0000000000000000\",\n",
      "  \"StreamId\": 0,\n",
      "  \"Metadata\": \"[ONNX Layer: /Range]\"\n",
      "},{\n",
      "  \"Name\": \"[trainStation3]\",\n",
      "  \"LayerType\": \"TrainStation\",\n",
      "  \"Inputs\": [],\n",
      "  \"Outputs\": [],\n",
      "  \"TacticValue\": \"0x0000000000000000\",\n",
      "  \"StreamId\": 0,\n",
      "  \"Metadata\": \"\"\n",
      "}],\n",
      "\"Bindings\": [\"x\"\n",
      ",\"y\"\n",
      "]}\n"
     ]
    }
   ],
   "source": [
    "!cat onnx_export_badcase.folded.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the layer /Rangeï¼Œ there is not input ...  \n",
    "```\n",
    "{\n",
    "  \"Name\": \"/Range\",\n",
    "  \"LayerType\": \"Fill\",\n",
    "  \"Inputs\": [],\n",
    "  \"Outputs\": [\n",
    "  {\n",
    "    \"Name\": \"y\",\n",
    "    \"Location\": \"Device\",\n",
    "    \"Dimensions\": [0],\n",
    "    \"Format/Datatype\": \"Row major linear Int32\"\n",
    "  }],\n",
    "  \"ParameterType\": \"Fill\",\n",
    "  \"Op\": \"LINSPACE\",\n",
    "  \"dimension\": [-2147483647],\n",
    "  \"alpha\": 0,\n",
    "  \"beta\": 1,\n",
    "  \"TacticValue\": \"0x0000000000000000\",\n",
    "  \"StreamId\": 0,\n",
    "  \"Metadata\": \"[ONNX Layer: /Range]\"\n",
    "}\n",
    "```\n",
    "- Are there some bugs in TensorRT during graph building?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
